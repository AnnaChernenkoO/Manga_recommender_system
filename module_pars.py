#для создания запроса к сайту Desu.me
import requests
#подключаем библиотеку для работы с файлами
import os
#подключаем библиотеку для работы с файлами формата CSV
import csv
from bs4 import BeautifulSoup
#библиотека для обработки данных файла CSV
import pandas as pd
#библиотека для правильного прописания пути к файлу CSV
import pathlib

#название файла с данными манги, а также путь к нему(находится в папке программы)
FILENAME = pathlib.Path(pathlib.Path.cwd(),"manga.csv")
#переменая для того чтобы прервать загрузку данных 
stop = False

#функция вызываемая в файле gui.py для изминения глобальной переменной stop
def stop_pars():
    #указываем, что переменная которая будет дальше использоваться глобальная, а не создается новая переменая
    global stop
    #меняем значение переменой stop, чтобы дальше в программе ее использовать в конструкции if, которая остановит парсинг(загрузку)
    stop = True


#функция для создания файла "manga.csv"
def create_csv():
    #создаем файл, если в папке уже есть такой же файл, то в переменую check_file передается значение True
    check_file = os.path.exists(FILENAME)

    #если check_file равен True, то существующий файл удаляется
    if check_file:
        #получаем полный путь к существующему файлу в переменной path
        path = os.path.join(os.path.abspath(os.path.dirname(__file__)), FILENAME)
        #передаем путь в функцию remove, чтобы удалить файл 
        os.remove(path)
    
    #эта конструкция создает файл и записывает зоголовки для наших данных. так же указываем в какой кодировке будет сохранятся файл, то есть в utf-8 
    with open(FILENAME, "w", newline="",encoding='utf-8') as file:
       writer = csv.writer(file)
       #прописываем зоголовки 
       writer.writerows([['TitleEng','TitleRus','Genre','Story','Rating']])

#функция для добавления данных в файл "manga.csv". в параметры функции передаем колекцию данных(manga), то есть название манги на англиском, на руском, жанры, описание и рейтинг 
def add_csv(manga):
    #открываем файл для записи данных 
    with open(FILENAME, "a", newline="",encoding='utf-8') as file:
       writer = csv.writer(file)
       #передаем колекцию данных которая запишется в файл
       writer.writerow(manga)

#функция для парсинга(записи данных с сайта в файл). принимает "progress_bar" для увеличения индикатора загрузки и
#"isp_fal" это функция из файла gui.py которая уведомляет интерфейс в файле gui.py о том, что загрузка окончена 
def pars_in_csv(progress_bar, isp_fal):

    #указываем, что будет использоваться дальше переменная stop которая была объявлена выше
    global stop
    #переменная при увеличении которой будет увеличиваться прогресс загрузки на интерфейсе
    x = 0
    #переменая для завершения загрузки данных. если на сайте в самом конце встретится 
    #элемент 'В данный момент список пуст.' то переменная изменит значение на False и закончит обработку
    tf = True

    #устанавливаем пргресс загрузки на начало
    progress_bar.UpdateBar(0)
    #вызываем функцию для создания файла "manga.csv"
    create_csv()

    #цикл для открытия последовательно страниц со списком манги на desu.me
    while(tf):
        #при переходе на следующую страницу со списком манги увеличиваем эту переменую на 1
        #чтобы отобразить прогресс загрузки
        x+=1
        #передаем переменую x в функцию отображающую загрузку на интерфейся для ее увеличения
        progress_bar.UpdateBar(x + 1)
        #создаем переменую адреса страницы. при увеличении x будет производится уже переход на другую страницу
        url = f'https://desu.me/manga/?page=' + str(x)
        #создаем запрос к сайту desu.me передавая ссылку на него
        response = requests.get(url)
        #получаем код страницы в переменую soup
        soup = BeautifulSoup(response.text, 'lxml')
        #ищем на странице сайта элемент имеющий класс 'animeList'
        #этот элемент появится только на последней странице списка манги
        #по нему можно отследить, что манги для записи в файл больше нет 
        empty = soup.find(class_='animeList')

        #получаем из выше объявленой переменной empty только текст и проверяем не равен ли он страке 'В данный момент список пуст.'
        #если не равен то парсин идет дальше, а если равен то ниже в конструкции else переменной tf 
        #присваивается False и выгрузка данных завершается
        if 'В данный момент список пуст.' != empty.div.text:
            #получаем каждую ссылку на страницу манги при помощи поиска класса animeTitle
            #для того чтобы зайти на каждую страницу и получить описание, жанр и рейтинг
            links = soup.find_all('a', class_='animeTitle oTitle')
            #с помощью цикла for проходим по коллекции полученных ссылок(links)
            #и последовательно получаем по одной ссылки(link) с каторой далее в цикле будем работать
            for link in links:
                #создаем запрос на получения страницы определенной манги
                page_response = requests.get('https://desu.me/'+link['href'])
                #получаем код страницы определенной манги
                page_soup = BeautifulSoup(page_response.text, 'lxml')

                #находим на странице англиское название манги
                name_a = page_soup.find('span', class_='name')
                #находим на странице русское название манги
                name_r = page_soup.find('span', class_='rus-name')
                #находим на странице рейтинг
                rating = page_soup.find('div', class_='score-value')
                #находим на странице жанры
                genre = page_soup.find_all('a', class_='tag')
                #находим на странице описание
                description = page_soup.find('div', class_='prgrph')
                #записываем в переменую только текст описания, без кода страницы
                des = description.text
                #убераем из текста переносы "\n", не нужные символы "\r" и не нужный текст
                des = des.replace("\n"," ").replace("\r"," ").replace("Манга удалена по просьбе издательства \"Истари Комикс\"", "")
             
                #создаем переменную жанра
                genres=""
                #Проходим в цикле по выше полученым жанрам и записываем все жанры через пробел в переменную genres
                for g in genre:
                    genres+=g.text+" "
                #создаем переменную в которой будет хранится колекция данных, состаящия из 
                #русского, англиского названия, жанров, описания и рейтинга манги
                manga =[name_a.text,name_r.text,genres,des,float(rating.text)]
                
                #производим запись в файл ранее полученых данных манги в файл с помощью ранее созданной функции
                add_csv(manga)

                #выход из цикла for если была нажата кнопка "прервать загрузку"
                if stop == True:
                    #выход из цикла
                    break

            #выход из цикла while
            if stop == True:
                #записывае в stop изначальное значение
                stop = False
                #сбрасываем индикатор загрузки в ноль
                progress_bar.UpdateBar(0)
                #уведомляем интерфейс, что загрузка данных в файл окончена
                isp_fal()
                #выход из цыкла
                break

        else:
            #записываем в tf False, что бы выйти из цикла
            tf = False
            #уведомляем интерфейс, что загрузка данных в файл окончена
            isp_fal()

#функция для получения количества страниц списка манги для для работы индикатора загрузки
def number_of_pages():
    #ссылка на сайт
    url = f'https://desu.me/manga/'
    #создаем запрос к сайту
    response = requests.get(url)
    #получаем код страницы
    soup = BeautifulSoup(response.text, 'lxml')
    #получаем элемент в котором указано количество страниц 
    text_html = soup.find('span', class_='pageNavHeader')
    #убираем разметку оставляем только текст
    text = text_html.text
    #обрезаем строку до момента где указана последняя страница 
    n = text.rsplit(' ', 2)
    #возвращаем количество страниц из функции
    return n[2]







